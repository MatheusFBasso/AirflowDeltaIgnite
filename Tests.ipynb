{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-29T01:38:23.496286Z",
     "start_time": "2025-10-29T01:38:22.267908Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "# Configure Spark session\n",
    "builder = SparkSession.builder \\\n",
    "    .appName(\"ReadBronzeTable\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.eventLog.enabled\", \"false\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.jars\", \"/opt/spark/jars/delta-spark_2.12-3.3.2.jar,/opt/spark/jars/delta-storage-3.3.2.jar,/opt/spark/jars/antlr4-runtime-4.9.3.jar\")\n",
    "\n",
    "# Apply Delta configuration\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "# Read Delta Table\n",
    "spark.sql('SELECT 1 AS a').show()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  a|\n",
      "+---+\n",
      "|  1|\n",
      "+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T01:38:56.035428Z",
     "start_time": "2025-10-29T01:38:55.712536Z"
    }
   },
   "cell_type": "code",
   "source": "spark.stop()",
   "id": "a818e783c9e51752",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from glob import glob\n",
    "\n",
    "for x in glob('../opt/airflow/data/warehouse/*'):\n",
    "    print(x)b"
   ],
   "id": "d6faa6444c77addf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "spark.stop()",
   "id": "45849fd2e310777b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "spark.read.format('delta').load('../opt/airflow/data/warehouse/bronze.db/divvy_bikes').show()",
   "id": "ffd5b131c4902159",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_bash_command(path_name:str) -> str:\n",
    "    bash_command = r\"\"\"\n",
    "            # Retrieve parameters\n",
    "            path=\"{{ params.path }}\"\"\" + path_name + r\"\"\"/\"\n",
    "            file_pattern=\"{{ params.file_pattern }}\"\n",
    "            n_days=\"{{ params.n_days }}\"\n",
    "\n",
    "            # Create path if it doesn't exist\n",
    "            mkdir -p \"${path}\"\n",
    "\n",
    "            # Generate timestamp\n",
    "            timestamp=$(date +%Y_%m_%d_%H_%M_%S)\n",
    "\n",
    "            # Create bkp subfolder if it doesn't exist\n",
    "            bkp_dir=\"${path}bkp\"\n",
    "            mkdir -p \"${bkp_dir}\"\n",
    "\n",
    "            # Debug: List files in path\n",
    "            echo \"Files in ${path}:\"\n",
    "            ls -l \"${path}\" || echo \"No files found or path error\"\n",
    "\n",
    "            # Count matching files\n",
    "            moved_count=$(find \"${path}\" -maxdepth 1 -type f -name \"${file_pattern}\" | wc -l)\n",
    "\n",
    "            # Move all matching files to bkp, appending timestamp to filename\n",
    "            find \"${path}\" -maxdepth 1 -type f -name \"${file_pattern}\" -exec sh -c '\n",
    "                base=$(basename \"$0\")\n",
    "                mv \"$0\" \"${1}/${base}_${2}\"\n",
    "            ' {} \"${bkp_dir}\" \"${timestamp}\" \\;\n",
    "\n",
    "            # Log moved files\n",
    "            if [ ${moved_count} -gt 0 ]; then\n",
    "                echo \"${moved_count} files moved to ${bkp_dir} successfully.\"\n",
    "            else\n",
    "                echo \"No files matching ${file_pattern} found to move.\"\n",
    "            fi\n",
    "\n",
    "            # Delete files in bkp older than n_days\n",
    "            deleted_count=$(find \"${bkp_dir}\" -type f -mtime +${n_days} | wc -l)\n",
    "            find \"${bkp_dir}\" -type f -mtime +${n_days} -delete\n",
    "\n",
    "            # Log deleted files\n",
    "            if [ ${deleted_count} -gt 0 ]; then\n",
    "                echo \"${deleted_count} files older than ${n_days} days in ${bkp_dir} deleted successfully.\"\n",
    "            else\n",
    "                echo \"No files older than ${n_days} days found in ${bkp_dir}.\"\n",
    "            fi\n",
    "            \"\"\"\n",
    "    return bash_command\n",
    "\n",
    "\n",
    "DivvyBikesPaths = {\n",
    "    'move_files_free_bike_status'    : get_bash_command(path_name='free_bike_status'),\n",
    "    'move_files_station_information' : get_bash_command(path_name='station_information'),\n",
    "    'move_files_station_status'      : get_bash_command(path_name='station_status'),\n",
    "    'move_files_system_pricing_plan' : get_bash_command(path_name='system_pricing_plan'),\n",
    "    'move_files_vehicle_types'       : get_bash_command(path_name='vehicle_types)')\n",
    "}\n",
    "\n",
    "print(DivvyBikesPaths.get('move_files_free_bike_status'))"
   ],
   "id": "2f9d4efe8aa25b2e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "rdd = spark.sparkContext.parallelize(data.get('data').get('stations'))",
   "id": "7f0b874eec056abe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "spark.read.option('inferSchema', False).json(rdd).show()",
   "id": "263aba7297876a89",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
